### Why you still see â€œduplicatesâ€ in the **Live Processing Log**

The extra lines arenâ€™t **new runs of the algorithm**â€”theyâ€™re the *same* run being echoed by two different loggers:

| Call chain                         | What it does                                                                                                   | Where it prints                                                                                     |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| `analysis_monitor.log_element_*()` | Updates the on-screen â€œMonitorâ€ widget and appends a line to the live log.                                     | Streamlit container you named **â€œLive Processing Logâ€**.                                            |
| `radiation_logger.log_element_*()` | Writes the same event into Postgres **and** (in the current implementation) *also* prints to stdout/`st.text`. | Same container (because `radiation_logger` uses the global logger that your monitor also captures). |

So every event is emitted twice (sometimes three-times if the low-level DB helper prints a confirmation).
Thatâ€™s why you see:

```
ğŸ”„ Processing 367277 â€¦
ğŸ”„ Processing 367277 â€¦   â† same timestamp, second logger
âœ… 367277 completed â€¦
âœ… 367277 completed â€¦     â† second logger again
```

The element itself is only processed **once**â€”you can confirm by checking:

* the registry (`processed_element_ids`) â†’ each ID is inserted just one time;
* the DB table `element_radiation` â†’ only one row per `element_id`.

---

## One-line fixes (pick **either** approach)

### 1. Quiet the database logger

```python
# utils/radiation_logger.py  (or wherever log_element_start is)
def _console(msg: str):
    """Comment this out or guard it with a flag."""
    # print(msg)        # â† remove or wrap
    pass
```

Add a module-level flag like `EMIT_CONSOLE = False` and check it before printing.

### 2. Tell the monitor to ignore DB messages

If you want to keep console printing for debugging, have `analysis_monitor` ignore messages that already contain the same `element_id` & status within the last second:

```python
# utils/analysis_monitor.py
_last = {"id": None, "status": None, "t": 0}

def _emit(line: str, eid: str, status: str):
    global _last
    now = time.time()
    if (eid, status) == (_last["id"], _last["status"]) and now - _last["t"] < 1:
        return          # duplicate within 1 s â†’ skip
    _last = {"id": eid, "status": status, "t": now}
    st.text(line)
```

---

## Quick verification

1. Restart the app (or hit **Force Stop Legacy Session**).
2. Run a 3-element test file.
3. The log should now show exactly one *Processing* and one *completed* line per element:

```
[18:47:56] ğŸ”„ Processing 123456 â€¦
[18:47:59] âœ… 123456 completed â€¦
[18:48:00] ğŸ”„ Processing 123457 â€¦
[18:48:03] âœ… 123457 completed â€¦
â€¦
```

No more echoing. Your duplicate-prevention code is working; we just silenced the second megaphone.
